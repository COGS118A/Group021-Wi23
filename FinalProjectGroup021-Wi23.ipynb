{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For beauty and cleanness, we do not include any code in the final report.**\n",
    "\n",
    "**Everything here is narrative.**\n",
    "\n",
    "#### Go to [CompleteProject](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb) for all complete code.\n",
    "\n",
    "Press CTRL/CMD and click the link to open a new tab, so you don't have to go back to this reoprt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the Music?\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Jerry Wang, A16149329\n",
    "- Darius Azure, A15786330\n",
    "- Miko Brown, A16237566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "Music has been created over thousands of years, people considered music as an expression of beauty, aesthetics, and art. However, it is rare to see people apply modern technology to analyze music. We want to explore the secret of music with a more scientific and computational way.\n",
    "\n",
    "The goal of our project is to classify song genre based on a variety of features describing the music. Our project tackles the problem of classifying song genre based on many variables including artist, song name, popularity, acousticness, danceability, energy, instrumentalness, key, tune, etc.\n",
    "\n",
    "The data contains entries with each of these features and their musical genre. We will create pipeline models to perform multi-classification on genre in the dataset. By comparing performance and metrics from different models, we will pick the highest accuracy algorithm by comparing the metrics from each one.\n",
    "\n",
    "In this project, we will select models pf Logstic Regression(naive tryout),Random Forest, Boosting, and Neural Network, and perform all of them in comparion.\n",
    "\n",
    "So, you will see three different models here: Random Forest led by Miko, Boosting led by Darius, Neural Network led by Jerry.\n",
    "\n",
    "In this project, we will use $f_1$ score to determine which model has the best performance. We will investigate precision and recall individually as well, but $f_1$ score will be the determining metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "In the history of music, there was no such complex system to define music genre in earlier times. It was more simple to categorize music according to its period of time<a name=\"wiki1\"></a>[<sup>[1]</sup>](#wikinote1). After the 20th century, creative musicians brought different format of music to the stage so that people start to categorize the music into genres. The music genre becomes more complicated as the technology develops. Nowadays, when facing countless music work, we cannot categorize a genre from its time. Instead, we may need to determine the music category based on its features, attributes, and listening response. \n",
    "\n",
    "A music genre or subgenre may be defined by the musical techniques, the cultural context, and the content and spirit of the themes<a name=\"wiki2\"></a>[<sup>[2]</sup>](#wikinote2). This article describes how many people subjectively view genre of music. This is useful for human-being, but difficult to classify large datasets of songs without listening to and understanding each song in depth. We can find that music genre itself is not easy to identified by individual factors from listening since people may have subjective thoughts. We tend to believe that there exists some more objective methods in machine learning to categorize music genre with computer algorithms.\n",
    "\n",
    "Additionally, music genre is a dynamic concept. Due to the natural evolution of music, it may appear differently in differnt periods of time. Therefore, a listening response might be outdated if a genre is gradually disappearing or shifting to different formats<a name=\"newyorker\"></a>[<sup>[3]</sup>](#newyorkernote). In contradiction, data implementation will not be affected by such evolution since data is measure objectively during that time period and measures a fixed in-time genre rather than a dynamic genre shifted from earlier times. For instance, the general keys of Jazz music in 1900s are different with Jazz music in 2020, but the computer will filter out the era's influence on keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Given different attributes of one song, what is its genre?\n",
    "\n",
    "The problem is quantifiable, we can represent the problem as:\n",
    "\n",
    "- Denote song entries $x_1$ to $x_n$ as X\n",
    "- Denote song genre $y_1$ to $y_n$ as y\n",
    "- Given $x_1$ to $x_n$, each $\\vec x_i$ $\\in \\mathbb{R}^d$ with features $f_1$ through $f_d$. What is $\\bar y_i$?\n",
    "\n",
    "ML-relevant potential solution: Give $x_1$ to $x_n$, by applying either LogisticRegression, RandomForest, Boosting, and Neural Network, the model produces the prediction of its most likely class $y_i$.\n",
    "\n",
    "The problem is measurable since we can evaluate the quantitative prediction performance with precision, recall, and f1 score.\n",
    "\n",
    "The problem is replicable since our selected algorithms are generally deterministic, once the data of a song is obtained, the problem can be reproduced by applying the same algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "- source: interest in music, general search online for datasets; author is from Wellington, New Zealand; the origin source of the data is unknown, we only grab it from Kaggle platform\n",
    "\n",
    "- reference: [Kaggle](https://www.kaggle.com/datasets/vicsuperman/prediction-of-music-genre), dataset search under classification\n",
    "\n",
    "- description dataset: 17 of variables, 49944 of observations\n",
    "\n",
    "- an observation consists of: instance_id, artist_name, track_name, popularity, acousticness, danceability,  duration_ms, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, obtained_date, valence, music_gengre\n",
    "\n",
    "- Actually we are not sure what should be considered as critical variables since any of the attributes of a song can make huge differernce to the song. Generally, the variables are presented in numerical values and categorical values and contains missing data.\n",
    "\n",
    "- cleaning of extreme numerical values, unmatched type, different format of categorical values\n",
    "\n",
    "- data missingness: if the data contains missing value, we need to first examine its type: Missing Completely At Random (MCAR), Missing At Random (MAR), or Missing Not At Random (MNAR). Then, we need to apply the strategy from one of the following: 1. ignore missingness 2. fill with special value 3.fill with mean/median 4.fill with nearest data 5.fill with our model prediction. This decision will be further determined in the data part if we encounter data missing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Why might your solution work? Make sure to describe how the solution will be tested.\n",
    "\n",
    "Given $x_1$ to $x_n$, each $\\vec x_i$ $\\in \\mathbb{R}^d$ with features $f_1$ through $f_d$. Predict \n",
    "the most likely class $y_i$.\n",
    "\n",
    "The solution is applicable and appropriate since:\n",
    "- Given features and observations, a multi-class classification is appropriate for this dataset since our true music genre has different categorical values.\n",
    "- We will build different models to produce the prediction, for example(not limited to) KNN, SVM, Neural Networks, Decision Trees, Gradient Boosting... The performance will be evaluated quantitatively.\n",
    "- We will use (repeated) K-fold cross validation to split test data and test our model performace with accuracy if the data size is not enough.\n",
    "- Comparision through models will be performed.\n",
    "\n",
    "Model selection:\n",
    "- We selected these models based on our multi-class classification(not limited to): LogisticRegression, RandomForest, Boosting, and Neural Network\n",
    "- We will use LogisticRegression as a first baseline model because it's predictive power is relatively low but it is easier to understand, we will present a trial and get a basic understanding of multi-label classification. Also, this is our very beginning learning from the lecture, so we decided to give a start.\n",
    "- RandomForest is a ensemble learning of decision trees, it is easy to understand and interpret. This will make our algorithm easier to visualize to give context to how it makes it's decision.\n",
    "- Boosting is another ensemble learning but different with bagging(forest), it allows us to improve and boost the weak learner from previous and focus on more mis-classified data.\n",
    "- Neural network is our exploration, NN is large and complicated. We want to implement a rough network within a medium size of neurons and present a solid output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "For our project we have chosen to focus on the evaluation metrics of precision, recall, and f1 score which we will use to try to evaluate how well our project is predicting the music genre. Due to having to store our data in a measurable way through our data cleaning to be quantifiable we need to see if we are handling these new variables are correctly being measured in a way where important comparisons have the proper weights. Since we need the project to be measurable we need to see that these variables that we cleaned are being weighed correctly which is why we will use f1 to try to compare our prediction of our training set to the actual genre to try to measure how many true postives and true negatives we have in our predictions in order to see how accurate our predictions are. We will use precision to determine how good we are at classifying a sample positively (quality of positive predictions). We will use recall to determine how good we are at detecting positive samples.\n",
    "\n",
    "We can also compare these scores to try to see if there is any overlap between predicting certain genres and see if our different models have more power in predicting these songs better than others depending on our score that we get. In general using these different factors that we have deemed quantifable we should check if we correctly measured the weight of these factors and see how truly we were able to identify the genre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "#### Section 3,4,5 are main models.\n",
    "\n",
    "### Subsection 1: Data and Clean\n",
    "We want to predict the music genre given the attributes from the music. Our data is from Kaggle. We will try to implement 3 different models and compare their performance: Random Forest, AdaBoosting, Neural Network.\n",
    "\n",
    "The dataset is relatively clean. There are only some missings in some random spots, which is completely blank, we interpret the reason as randomling missing or lack of information. So, we decide to drop all missing values.\n",
    "\n",
    "Here, take a look at our data and the cleaning: [Data and Clean](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#Data)\n",
    "\n",
    "### Subsection 2: Features and Visualization\n",
    "\n",
    "Firstly, we drop out the obtain date in the dataset since it is completely unrelated to our problem.(obtain date means the date when the collector receives the music data)\n",
    "\n",
    "We have 17 features in total, but we decided to abandon some of them. Here is a quick introduciton:\n",
    "- **Quantitaive** feature: popularity, acousticness, danceability, duration_ms, energy, instrumentalness, liveness, loudness, mode, speechiness, tempo, valence, 12 features in total\n",
    "- **Categorical** feature: key, mode, 2 features in total\n",
    "- **Abandoned Unrelated** feature: instance_id, artist_name, track_name, 3 features in total\n",
    "\n",
    "We categoize these 3 features as unrelated since they are just the names/composer names/ids.\n",
    "\n",
    "Numerial features are always good to keep.\n",
    "\n",
    "Categorcial features:\n",
    "\n",
    "- **Key**: After we visualize the key and its music genre, we find that the genres are distributed evenly in every key. So, we believe that the feature key will not improve any of the models since it lacks of information and independence. In conclusion, we will not use key as one feature. [Here](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#histogram) is our stacked histogram, **please** take a look, <span style=\"color:red\">it is really **nice**!</span>.\n",
    "- **Mode**: A feature only contains two possible values, easy to encode into 0 and 1, it has solid difference and independence, we will keep it.\n",
    "\n",
    "Then, we plot the [correlation](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#heatmap) and want to find out the max and min correlation, max:0.84, min:-0.79 which is acceptable, means that every feature is not strictly correlated. It is fine for us to keep every numerical feature.\n",
    "\n",
    "#### Subsection 2+: [Naive Tryout from Lecture](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#First-Naive-Attempt:-Logistic-Regression)\n",
    "After cleaning our data we decided to use a baseline to run through our data where we chose to do Logistic Regression. Due to this being a 10-label classification problem we knew that the accuracy was not going to be done well by an algorithm like this due to not every variable having a linear relationship. Despite this we knew with a large dataset it would be a good baseline view of testing out our data to compare with our later three models. \n",
    "\n",
    "We used a default set of values and ran it with multinomial l2 penalty where we got our f1 score of [29.6%](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#logisticresult) accuracy which while not good was a good baseline to compare to our models that we expect to improve on the accuracy. So while this accuracy is not good in any means of the word, it is a good baseline of the performance.\n",
    "\n",
    "### Subsection 3: [Random Forest](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#Random-Forest)\n",
    "We cleaned up the data, and dropped non-relevant columns. We encoded mode as integer 0 or 1 so that it could be valid numeric input for the classifier.\n",
    "\n",
    "We then created a train test split of the data and performed a grid search on three hyperparameters: number of trees, maximum features, and max_depth. We ran grid search over the values 100, 200, 500 for number of trees, sqrt or log2 for maximum features, and 5 or 10 for max_depth. The best parameters were 500, sqrt, and 10.\n",
    "\n",
    "Taking the best parameters from the grid search, we got an f1 score of [0.573227](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#rf-acc), much higher than the baseline model of logistic regression with a 0.29 f1 score.\n",
    "\n",
    "\n",
    "### Subsection 4: [AdaBoosting](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#AdaBoosting)\n",
    "We then decided to do AdaBoosting to try out another example of ensemble learning which of course has its own hyperparameters. Oftentimes altering one of these parameters alters the accuracy and as a result we needed to test out different selections of parameters.\n",
    "\n",
    "N_estimators: we had decided on doing the values 50,100 and 150 to try out editing the number of estimators to use in order to learn from these weaker models to try to find a sweet spot. Learning_rate: This parameter which controls how fast or slow the algorithms runs can also alter how fast the models will learn from each other to find the loss and works together with the n_estimators to find this weight. We used the ranges of 0.001, 0.1 and 1. Estimator: We chose the default of decision trees as we also tried SVM but never got it to finish.\n",
    "\n",
    "We then used a GridSearch to find the best combination of these and found the best was a learning rate of 0.1 and n_estimators of 150. From there after running we found that our accuracy was [0.525](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#adab-acc) . By using these parameters we have found our best from these options but we could always try running with different parameters but often they will have lower accuracies resulting in this being our best estimate. \n",
    "\n",
    "### Subsection 5: [Neural Network](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#Neural-Network)\n",
    "After the ensemble learning, we decide to try out neural network. We firstly encode y labels into binary matrix and round every numerical values to 3 digits. \n",
    "\n",
    "We decided to have 1 or 2 hidden layers in the network since we believe the data since hidden layers are needed if and only if the data are nonlinearly separated. The **core** of neural network is to tune hyperparameters well enough. At first, we decided to use the [grid search](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#FinalProjectCompleteVersion.ipynb#n-gridsearch) to find appropriate batch size and epoch. However, this takes up so long time to run that we cannot afford to perform additional search on optimizer, momentums, and etc. Those hyperparameters are the key to improve performance. Unfortunately, the time limit locks us down. We end up doing one time of search and perform the network about [0.54](https://nbviewer.org/github/COGS118A/Group021-Wi23/blob/main/FinalProjectCompleteVersion.ipynb#nn-acc) accuracy and $f_1$ score. We believe that the performance of neural network will be better and might be close to our ultimate goal: over 70%.\n",
    "\n",
    "### Subsection 6: Outcomes Comparison\n",
    "Three models and accuracy:\n",
    "\n",
    "- RandomForest: 0.573\n",
    "\n",
    "- Adaboosting: 0.525\n",
    "\n",
    "- Neural Network: 0.535\n",
    "\n",
    "They are Acceptable results, but definitely can have improvements. Before we begin this project, we have little idea of the difficulty to classify 10 labels, especially with some \"similar\" features. Now, we have a understanding:\n",
    "\n",
    "- Some attributes are super representative, sometimes one song is consistent or inconsistent of features, which leads to a classification perturbation.\n",
    "\n",
    "- Any of the search for hyperparameters takes long time to run, we should start earlier and tune for more trials.\n",
    "\n",
    "- We still need to learn a lot in machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the Result\n",
    "\n",
    "Classifying genre based on the features provided, even with our best model, was difficult to obtain high(70%+) accuracy and meet public expectations of a classifier.\n",
    "\n",
    "It is difficult to reach a high level of accuracy for 10-label classification because there are so many possible classes. Genre is a problem that makes this difficult because songs often belong to multiple genres or some genres are obscure. This means that while each of the songs are clearly labelled as a single genre, the differences in their input features may be very minimal as the line between different genres can be hard to determine. We tried different ensembles such as random forest and adaboost and even a neural network but none of the models could reach greater than 0.6 $f_1$ score. We likely get stuck at this point because the input features are similar across genres.\n",
    "\n",
    "After analyzing our results, we were satisfied of the increase in accuracy we got, going from 0.29 $f_1$ score in our baseline logistic regression model to 0.573227 in our best model, the random forest. While we did increase our accuracy significantly through hyperparameter tuning for each model, we are aware that if the public were to use our classifier, it would likely fall short of their expectations because they would want high accuracy. \n",
    "\n",
    "Time limit is our main concern: optimizing models takes much more time than we expected. We think that with more time, we could have gotten closer to over 70%, which is our ultimate goal.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- We had limited time to run and visualize all of the models. We could likely increase model performance with more in-depth hyperparameter tuning if we had more time to run longer. Running grid search over increasing number of parameter selections increasing compute time very quickly so we had to only cover relatively few values for each hyperparameter.\n",
    "\n",
    "- The tree was large and the neural network was very large, which made it hard to visualize the models purely due to their size. Visualizations were smushed together and clunky due to the size.\n",
    "\n",
    "- Our best end result performance was around 0.574. While this is not bad given 10 classes, it likely falls short of people's expectations for a classifier where many people would expect over 80% accuracy.\n",
    "\n",
    "- People’s exact definitions of genre are constantly evolving making it hard for a machine to exactly classify genre since it is an issue humans have. Due to this complexity it makes it difficult and nearly impossible to perfectly classify just using a machine. As a result it makes it not very convincing to be able to perfectly label a genre and leaves plenty of room for error making it a limitation to our model.\n",
    "\n",
    "### Ethics & Privacy\n",
    "Due to the fact that each song labeled with artist and the name of the song there is the problem of these being identifable and since this data is likely not important to our analysis we will not reflect specific songs in our analysis and try to avoid biases due to song selections in our fold selection to the best of our ability. We will have our data highlight only the important aspects of the analysis that will not have identifiable links back to the data. We will have the songs instead represented as meaningless id code labels that will have no relation to the song or artist themselves to avoid potential bias in our data analysis and will instead focus on the other factors about the songs to focus instead on predicting the genre based on those quantifiable aspects rather than inherit biases we may unintentionally have if we based it on song or artist names. Due to the nature of us scrapping data that we ourselves did not collect we will comb through the data to make sure that there are no ethical issues with the data collection and that there is no bias to what was collected and that there is no PIID values that may have been collected in the data as discussed earlier. We will make sure that all genres are properly represented in valid ways and that our data analysis relys on purely numerical data that is comparable rather than our own bias. Furthermore if any ethical dilemas come up during or after our research we plan on adjusting our data to handle or remove ethical concerns within our modeling and deployment of the analysis. We will have our data in comparable and categorical matter where we weigh our data properly but if these factors are found to not be properly utilized or have a bias that affects our ethics we will try to reflect this in our final project.\n",
    "\n",
    "Also, classifying music genre is a process about classifying music and making people more aware of music, which does not involve any extreme,  negative,  or positive evaluations. All output results, regardless of real facts, are objective from computer evaluation and will not negatively mislead society or the public. Further, we will also state that this scientific classification is only used as a reference in machine learning. When people are really willing to try to get in touch with music genre, they should consider everything comprehensively from both objective facts and subjective opinions. \n",
    "\n",
    "We all believe that **Appreciation and Analysis of Music requires both rationality and sensibility**.\n",
    "\n",
    "### Conclusion\n",
    "Trying to reach a high accuracy with 10-label classification is not easy, with our three models we were not able to get our accuracy over 60%. Our highest accuracy being the Random Forest shows that we can find a decent accuracy that averages around 55% between our three models but is still not a perfect representation. However, this even connects to another problem with the main point in general as classifying music genres is tough for a human to do, let alone a machine learning algorithm. In general this shows based on our data that considering the issue of classifying an ever changing concept we have a manageable accuracy that we could only improve on. \n",
    "\n",
    "Some future work we can do would be to try to test specific factors against each other or try out different scenarios like SVM as our estimator for AdaBoosting, trying out XGBoost, and further training our neural network that we did not have the time for in a classroom setting. We believe that while we did not reach our goal of 70% accuracy that we have still found a good understanding of our data and have trained our models well to represent a good enough accuracy of classification with our 10 genre labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"wikinote1\"></a>1.[^](#wiki1): (16 February 2023) History of Music Wikipedia. https://en.wikipedia.org/wiki/History_of_music<br>\n",
    "<a name=\"wikinote2\"></a>2.[^](#wiki2): (9 February 2023) Music genre Wikipedia https://en.wikipedia.org/wiki/Music_genre#:~:text=A%20music%20genre%20or%20subgenre,a%20wide%20variety%20of%20subgenres<br>\n",
    "<a name=\"newyorkernote\"></a>3.[^](#newyorker): (15 March 2021) Genre Is Disappearing. What Comes Next? https://www.newyorker.com/magazine/2021/03/15/genre-is-disappearing-what-comes-next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
